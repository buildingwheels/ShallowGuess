# Training Guide

## Prerequisites

Before starting the training process, ensure you have:
- Python with required dependencies (basically just PyTorch and its dependencies)
- PGN data for training

## Training Process

### 1. Filter FEN Data

First, run the FEN data filtering script to clean and prepare your data:

```bash
cargo run --bin filter_fen_data [input_file] [output_file] [max_count] [skip_terminated_game]
```

**Parameters:**
- `input_file`: Path to the FEN data file generated by `pgn-extract`
- `output_file`: Path where the filtered FEN data will be saved
- `max_count`: Maximum number of positions to include per game (e.g., 200 to limit game length)
- `skip_terminated_game`: Boolean flag (true/false) to skip games that terminated abnormally

**Example:**
```bash
cargo run --bin filter_fen_data data/extracted_fens.txt data/filtered_fen.txt 200 true
```

**Note:** Before running this script, you need to extract FEN positions from your PGN files using pgn-extract:
```bash
pgn-extract -Wfen -o data/extracted_fens.txt data/games.pgn
```

### 2. Generate Training Data

Next, generate the training data from the filtered FEN files:

```bash
cargo run --bin gen_training_data [input_file] [output_file] [batch_size] [optional_log_file]
```

**Parameters:**
- `input_file`: Path to the filtered FEN data file (output from step 1)
- `output_file`: Path where the training data will be saved
- `batch_size`: Number of positions to buffer before writing to disk (e.g., 10000)
- `optional_log_file`: (Optional) Path to a log file for debugging filtered positions

**Example:**
```bash
cargo run --bin gen_training_data data/filtered_fen.txt data/training_data.bin 10000 data/filtering.log
```

### 3. Train the Model

Now you can train the neural network using the generated training data:

```bash
python training_scripts/trainer.py [hidden_layer_size] [data_dir] [model_export_path] [max_epochs] [sample_size] [options]
```

**Required Parameters:**
- `hidden_layer_size`: Size of the hidden layer in the neural network (e.g., 512)
- `data_dir`: Directory containing training data files
- `model_export_path`: Directory where trained models will be saved
- `max_epochs`: Maximum number of training epochs (e.g., 100)
- `sample_size`: Number of data files to randomly sample per epoch

**Optional Parameters:**
- `--batch_size`: Batch size for training (default: 1024)
- `--learning_rate`: Learning rate for optimization (default: 0.001)
- `--print_cycle`: Number of batches between progress updates (default: 100)
- `--export_cycle`: Number of batches between model exports (default: 1000)
- `--existing_pth_file`: Path to existing model file to continue training
- `--enable_diagnostics`: Enable gradient and weight change diagnostics (default: False)
- `--max_pos_count`: Max position count for weight scaling (default: 400)
- `--training_log_file`: Path to the training log file

**Training Details:**
- **Optimizer**: AdamW with weight decay=1e-4
- **Learning Rate Scheduler**: ReduceLROnPlateau (factor=0.1, patience=1)
- **Loss Function**: Weighted Cross Entropy (weights based on position count)

**Example:**
```bash
python training_scripts/trainer.py 512 data/training/ resources/models/ 50 10 --batch_size 64 --learning_rate 0.05
```

### 4. Export and Build Weights

After training is complete, export the model weights and build them for use in the engine:

```bash
./build_scripts/build_weights.sh [hidden_layer_size]
```

**Build Script Parameters:**
- `hidden_layer_size`: Size of the hidden layer (must match training)

**Example:**
```bash
./build_scripts/build_weights.sh 512
```
